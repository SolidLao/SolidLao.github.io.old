---
title: "From DB-BERT to DB-BART and Beyond"
last_modified_at: 2023-10-20T16:20:02-05:00
categories:
  - Knob
tags:
  - Database Knob Tuning
  - Pre-trained Language Model
  - DB-BERT
classes: wide
---

## Brief Introduction to Database Knob Tuning
Modern database management systems (DBMS) expose hundreds of configurable parameters (i.e., knobs) to control system behaviors.
Selecting appropriate values for these knobs is crucial to improve DBMS performance (e.g., set knob 'shared_buffers' from PostgreSQL to 25% of the RAM to improve performance).

![DBMS-Knob-Tuning](/assets/images/knob_tuning.png){: width="40%"}

As a common practice, experienced Database Administrators (DBAs) take great efforts (e.g., weeks or even months) to tune the target DBMS for a given workload (i.e., a workload is a set of SQL statements).

![DBA-Tuning](/assets/images/human_tuning.png){: width="40%"}

However, manual tuning struggles to handle different workloads, hardware environments, especially in nowadays cloud environments. Thus Machine Learning (ML)-based tuning systems are proposed to search for well-performing knob configurations automatically. They can be classified into two main categories: Bayesian Optimization (BO)-based and Reinforcement Learning (RL)-based. The algorithm details are omitted here, please refer to [OtterTune](https://db.cs.cmu.edu/projects/ottertune/) and [CDBTune](https://dl.acm.org/doi/10.1145/3299869.3300085) for more details, which are the representative works for BO-based and RL-based methods, respectively. 

## The Limitations of ML-based Tuning Systems
While ML-based DBMS tuning methods do possess the potential to eventually reach well-performing knob configurations, they often come at the cost of being resource-intensive and requiring a considerable number of iterations to achieve convergence (i.e., each iteration requires the stress-test for target workload on DBMS). Such high costs come from the complexity of the tuning problem: the configuration space is high-dimensional (e.g., PostgreSQL v14.7 has 351 knobs) and heterogeneous (e.g., a knob can be continual or categorical), and optimization algorithms struggle with this complexity. Instead of searching the configuration space only based on runtime feedback (i.e., black-box optimization), we can enhance the tuning process with domain knowledge because DBMS itself is domain-specific and there is extensive domain knowledge available from DBMS official documents, manuals, web forums. However, since such domain knowledge is expressed in natural language and seems exclusive to DBAs, the natural question is **how to make the knowledge understandable to machines and use the gained information to enhance knob tuning process?**

## DB-BERT: a Database Tuning Tool that ``Reads the Manual''
[DB-BERT](https://itrummer.github.io/dbbert/) is an excellent work that utilizes a pre-trained language model ([BERT](https://huggingface.co/docs/transformers/model_doc/bert)) 

## From DB-BERT to DB-BART and DB-GPT-4


## Beyond DB-BERT Series: GPTuner


## Comparison between DB-BERT Series and GPTuner